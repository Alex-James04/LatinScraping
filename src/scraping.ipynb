{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a91661f",
   "metadata": {},
   "source": [
    "# Latin Texts Web Scraping\n",
    "This file contains the code used to scrape PHI Latin Texts and produce the resulting parquet file used in the final Power BI report.\n",
    "Because all the code in this file was the result of experimenting and becoming familiar with the Beautiful Soup library, the program is currently very innefficient.\n",
    "No AI was used in the making of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75fd75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Headers for device used to scrape - may need to update if run on other devices\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4eac1f",
   "metadata": {},
   "source": [
    "### Initial Request\n",
    "This cell makes a request to PHI Latin Texts' homepage in order to scrape a list of authors to be scraped from further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9827671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base request to the page containing the list of authors and links to their pages\n",
    "response = requests.get('https://latin.packhum.org/browse', headers=headers)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Getting every link on the page and saving all links that point to authors' pages to a list\n",
    "homepage_links = soup.find_all('a')\n",
    "homepage_links_references = []\n",
    "for tag in homepage_links:\n",
    "    homepage_link_reference = tag.get('href')\n",
    "    if 'author' in homepage_link_reference:\n",
    "        homepage_links_references.append(homepage_link_reference)\n",
    "\n",
    "# Getting every author's name associated with each link scraped above\n",
    "author_tags = soup.find_all(attrs={\"class\": \"srch\"})\n",
    "author_names = []\n",
    "for tag in author_tags:\n",
    "    author_names.append(tag.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5644020",
   "metadata": {},
   "source": [
    "### Scraping Authors for their Works\n",
    "This cell loops over the list of authors and their links relative to the base URL to get a list of the works, or texts, that will be scraped later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a4b857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list that will store the author information from above, alongside each author's work's title and link\n",
    "works = []\n",
    "\n",
    "# Loop going over links/names scraped above\n",
    "for author_index in range(len(homepage_links_references)):\n",
    "\n",
    "    # Request to each author's respective page\n",
    "    response = requests.get('https://latin.packhum.org' + homepage_links_references[author_index], headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Getting every link on the page and saving all links that point to unique works to a list\n",
    "    works_links = soup.find_all('a')\n",
    "    works_links_references = []\n",
    "    for tag in works_links:\n",
    "        works_links_reference = tag.get('href')\n",
    "        if 'loc' in works_links_reference:\n",
    "            works_links_references.append(works_links_reference)\n",
    "\n",
    "    # Getting every work's name associated with each link scraped above\n",
    "    works_tags = soup.find_all(attrs={\"class\":\"wnam\"})\n",
    "    works_names = []\n",
    "    for tag in works_tags:\n",
    "        works_names.append(tag.text)\n",
    "\n",
    "    # Saving the results of the scraping for the current author to the works list for future reference\n",
    "    for work_index in range(len(works_links_references)):\n",
    "        works.append([author_names[author_index], works_links_references[work_index], works_names[work_index]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c321ff2",
   "metadata": {},
   "source": [
    "### Scraping Texts\n",
    "This cell loops over all pages containing raw Latin texts and saves the entire text as a string. Currently, this cell takes ~3 hours to run on my PC - again, since this project was about experimenting, I now can see that this is wildly inefficient and can likely be done much faster (although I am assuming the bulk of that time is unavoidable due to making 800+ requests)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85a0d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list that will store the author and work information from above, alongside each work's raw text as a string\n",
    "texts = []\n",
    "\n",
    "# Loop going over links/works scraped above\n",
    "for work_index in range(len(works)):\n",
    "\n",
    "    \"\"\"\n",
    "    Some authors' works are so large that they are stored on multiple pages on the website being scraped.\n",
    "    This sub-loop goes through every sub-page storing multiple pages.\n",
    "    This solution works by storing all pages' texts to a single cumulative text, with the loop being broken if the current page's text is already in the cumulative text (final page reached).\n",
    "    \"\"\"\n",
    "\n",
    "    # Defining cumulative text and increment used to iterate through multiple sub-pages' urls\n",
    "    cumulative_text = \"\"\n",
    "    page_increment = 0\n",
    "\n",
    "    # Multi-page loop\n",
    "    while True:\n",
    "\n",
    "        # Request to current page/sub-page\n",
    "        response = requests.get('https://latin.packhum.org' + works[work_index][1][:-1] + str(page_increment), headers=headers)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        \"\"\"\n",
    "        Each page containing text has the text split into multiple sections, with all sections with the \"td\" tag being the raw text.\n",
    "        This loop Gets all \"td\" tags and stores their individual texts to a single string containing the full page's text (which is then added to the cumulative text).\n",
    "        \"\"\"\n",
    "\n",
    "        # Defining full text and finding al \"td\" tags\n",
    "        full_text = \"\"\n",
    "        text_sections = soup.find_all('td')\n",
    "\n",
    "        # Loop combining all \"td\" tag texts\n",
    "        for text in text_sections:\n",
    "            full_text = full_text + \" \" + text.text.strip()\n",
    "\n",
    "        # Check for if the current page's text is already in the cumulative text - indicates the previous iteration was the final page for the work and that the inner loop should be terminated\n",
    "        if full_text in cumulative_text:\n",
    "            break\n",
    "\n",
    "        # Updating cumulative text with full text and incrementing the page for next iteration\n",
    "        cumulative_text += \" \" + full_text\n",
    "        page_increment += 1\n",
    "\n",
    "    # Saving current work's full (potentially multi-page) text to the texts list with the scraped data from the previous cell\n",
    "    current_work_copy = works[work_index].copy()\n",
    "    current_work_copy.append(cumulative_text)\n",
    "    texts.append(current_work_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bfe44f",
   "metadata": {},
   "source": [
    "### Creating Word Dictionary\n",
    "This cell loops through each saved text and creates a dictionary containing every unique word in the text and their counts throughout the texts - this will be the basis for most of the final Power BI report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9f068e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop to turn raw text stored in list above into dictionary containing each unique word and their respective counts for each text\n",
    "for text in texts:\n",
    "\n",
    "    # Adjusting formatting of scraped data stored in current text's array (author names and links for readability/usage, words for formatting as dictionary)\n",
    "    text[0] = re.sub(r'\\s+', ' ', re.sub('[^a-zA-Z]', ' ', text[0])).strip()\n",
    "    text[1] = 'https://latin.packhum.org' + text[1]\n",
    "    word_list = re.sub(r'\\s+', ' ', re.sub('[^a-z]', ' ', text[3].lower())).strip().split()\n",
    "\n",
    "    # Creating word dictionary and adding each word in current text, as well as updating dictionary's count if the current word was already added in current text\n",
    "    word_dictionary = {}\n",
    "    for word in word_list:\n",
    "        if word in word_dictionary:\n",
    "            word_dictionary[word] += 1\n",
    "            continue\n",
    "        word_dictionary[word] = 1\n",
    "\n",
    "    # Adding dictionary to current text's array of scraped data\n",
    "    text.append(word_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19ffd5e",
   "metadata": {},
   "source": [
    "### Manipulating/Formatting Results\n",
    "This cell uses Pandas to format the data stored in the texts list into a Dataframe/spreadsheet that can be easily exported (or analyzed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ade4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating final DataFrame to store all scraped data\n",
    "df = pd.DataFrame(texts, columns=['author', 'source', 'title', 'text', 'word'])\n",
    "\n",
    "# Turning column containg dictionary of words and their counts into two distinct columns - one for the word in the given text, and the other for the count of that word in the given text\n",
    "df['word'] = df['word'].apply(lambda row: list(row.items()))\n",
    "df = df.explode('word')\n",
    "df[['word', 'count']] = df['word'].apply(pd.Series)\n",
    "\n",
    "# Dropping full raw text - WHOLE PROGRAM COULD BE IMPROVED IF THIS WAS DONE EARLIER (leaving as is because scraping theoretically only needs to be done once)\n",
    "textless_df = df.drop(columns='text')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf8fa92",
   "metadata": {},
   "source": [
    "### Exporting\n",
    "This cell has two ways of exporting the resulting dataframe commented out - to save the results to either csv or parquet, uncomment the respect line.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9eaac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save result to csv (too large to push to GitHub)\n",
    "# textless_df.to_csv('../bin/scraped_latin_texts_words.csv', index=False)\n",
    "\n",
    "# Save result to compressed Parquet for GitHub push\n",
    "# textless_df.to_parquet('../bin/scraped_latin_texts_words.parquet', index=False, compression=\"gzip\", engine=\"pyarrow\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
